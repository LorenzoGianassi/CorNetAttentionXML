{
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Correlation Networks for Extreme Multi-label Text Classification in Google Colaboratory\n",
        "This is a Python implementation of the paper  \"[Correlation Networks for Extreme Multi-label Text Classification](https://www.semanticscholar.org/paper/Correlation-Networks-for-Extreme-Multi-label-Text-Xun-Jha/2528e161a0e2d4bdb2b0482ec5866a3914d0758b)\" by *Guangxu Xun, Kishlay Jha, Jianhui Sun, Aidong Zhang*.\n",
        "\n",
        "## Baselines\n",
        "This implementation is based on the work by [XunGuangxu](https://github.com/XunGuangxu) and the codes for the baseline model is adapted from the following repository available [here](https://github.com/XunGuangxu/CorNet).\n",
        "\n",
        "# Acknowledgments\n",
        "Machine Learning Project Â© Course held by Professor [Paolo Frasconi](https://www.unifi.it/p-doc2-2016-200006-F-3f2a3d2f332b2c-0.html) - Computer Engineering Master Degree @[University of Florence](https://www.unifi.it/changelang-eng.html)"
      ],
      "metadata": {
        "id": "6bkL1Ruiq1bs"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LsJvMN15lE_d"
      },
      "source": [
        "Save the directory where the saved models are, so I always have the shared drive with all my model parameters."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WIpCRO-BjpyJ"
      },
      "outputs": [],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/gdrive', force_remount=True)\n",
        "\n",
        "base_dir = \"/content/gdrive/MyDrive/Machine_Learning/\" # Specify your Drive dir where to load/save models"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lgUJLd6E5tQM"
      },
      "source": [
        "#Datasets\n",
        "Below we will extract and manage the datasets that have been used for the experiments.\n",
        "We use two benchmark datasets:\n",
        "\n",
        "1.   One medium-scale dataset: ***AmazonCat-13k***\n",
        "2.   One small-scale dataset: ***EUR-Lex***\n",
        "\n",
        "For each dataset, the vocabulary size is limited to 500,000 words according to the word frequency in the training set.\n",
        "\n",
        "Word embeddings are initialized with the 300-dimensional pretrained GloVe embeddings. Word embeddings are frozen for the EUR-Lex dataset.\n",
        "\n",
        "Input text sequences are truncated to 500 words if longer. \n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-wUx44071nD9",
        "outputId": "1716e88d-0f10-49b1-abba-5761b992e18f"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n"
          ]
        }
      ],
      "source": [
        "import re\n",
        "import nltk\n",
        "nltk.download('punkt')\n",
        "from nltk.tokenize import word_tokenize\n",
        "\n",
        "def tokenize(sentence: str, sep='/SEP/'):\n",
        "    # We added a /SEP/ symbol between titles and descriptions such as Amazon datasets.\n",
        "    return [token.lower() if token != sep else token for token in word_tokenize(sentence)\n",
        "            if len(re.sub(r'[^\\w]', '', token)) > 0]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fQm6t-SW4ABJ"
      },
      "outputs": [],
      "source": [
        "from typing import Union, Iterable\n",
        "from gensim.models import KeyedVectors\n",
        "from collections import Counter\n",
        "import numpy as np\n",
        "def build_vocab(texts: Iterable, w2v_model: Union[KeyedVectors, str], vocab_size=500000, # with union the variable accepts more types\n",
        "                pad='<PAD>', unknown='<UNK>', sep='/SEP/', max_times=1, freq_times=1):\n",
        "    if isinstance(w2v_model, str):\n",
        "        w2v_model = KeyedVectors.load(w2v_model)\n",
        "    emb_size = w2v_model.vector_size\n",
        "    vocab, emb_init = [pad, unknown], [np.zeros(emb_size), np.random.uniform(-1.0, 1.0, emb_size)]\n",
        "    counter = Counter(token for t in texts for token in set(t.split())) # count the occurecies of each token in a dictionary\n",
        "    for word, freq in sorted(counter.items(), key=lambda x: (x[1], x[0] in w2v_model), reverse=True):\n",
        "        if word in w2v_model or freq >= freq_times:\n",
        "            vocab.append(word)\n",
        "            # We used embedding of '.' as embedding of '/SEP/' symbol.\n",
        "            word = '.' if word == sep else word\n",
        "            emb_init.append(w2v_model[word] if word in w2v_model else np.random.uniform(-1.0, 1.0, emb_size))\n",
        "        # if the word occurs more than once or if the size of the vocabulary is equal to 500k we have a break\n",
        "        if freq < max_times or vocab_size == len(vocab):  \n",
        "            break\n",
        "    return np.asarray(vocab), np.asarray(emb_init)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ta_KaCQx5p-0"
      },
      "outputs": [],
      "source": [
        "from tqdm import tqdm\n",
        "import joblib\n",
        "from sklearn.preprocessing import MultiLabelBinarizer\n",
        "def truncate_text(texts, max_len=500, padding_idx=0, unknown_idx=1):\n",
        "    if max_len is None:\n",
        "        return texts\n",
        "    texts = np.asarray([list(x[:max_len]) + [padding_idx] * (max_len - len(x)) for x in texts])\n",
        "    texts[(texts == padding_idx).all(axis=1), 0] = unknown_idx\n",
        "    return texts\n",
        "\n",
        "def convert_to_binary(text_file, label_file=None, max_len=None, vocab=None, pad='<PAD>', unknown='<UNK>'):\n",
        "    with open(text_file) as fp:\n",
        "        texts = np.asarray([[vocab.get(word, vocab[unknown]) for word in line.split()] for line in tqdm(fp, desc='Converting token to id', leave=False)])\n",
        "    labels = None\n",
        "    if label_file is not None:\n",
        "        with open(label_file) as fp:\n",
        "            labels = np.asarray([[label for label in line.split()]\n",
        "                                 for line in tqdm(fp, desc='Converting labels', leave=False)])\n",
        "    return truncate_text(texts, max_len, vocab[pad], vocab[unknown]), labels\n",
        "\n",
        "\n",
        "# method to return the word embedding\n",
        "def get_word_emb(vec_path, vocab_path=None):\n",
        "  if vocab_path is not None:\n",
        "      with open(vocab_path) as fp:\n",
        "          vocab = {word: idx for idx, word in enumerate(fp)}\n",
        "      return np.load(vec_path), vocab\n",
        "  else:\n",
        "      return np.load(vec_path)   \n",
        "\n",
        "      \n",
        "# method that trains and binarize the labels if. doesn't already exist at the path, otherwise create it.\n",
        "def get_mlb(mlb_path, labels=None) -> MultiLabelBinarizer:\n",
        "    if os.path.exists(mlb_path):\n",
        "        return joblib.load(mlb_path)\n",
        "    mlb = MultiLabelBinarizer(sparse_output=True)\n",
        "    mlb.fit(labels)\n",
        "    joblib.dump(mlb, mlb_path)\n",
        "    return mlb      "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FOQrF0O09iGh"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "\n",
        "\n",
        "def preprocessing(text_path=None, tokenized_path=None, label_path=None, truncate_label_path=None, vocab_path=None, emb_path=None, w2v_model=None, vocab_size=500000, max_len=500, truncate=False, max_lines=100000):\n",
        "    number_lines_text=0\n",
        "    number_lines_labels=0\n",
        "\n",
        "    # decrease number of elements in the labels\n",
        "    if truncate is True and truncate_label_path is not None :\n",
        "        print('Decreasing the size of the labels...')\n",
        "        with open(label_path) as lb, open(truncate_label_path, 'w') as lbout:\n",
        "              for line in tqdm(lb, desc='Decreasing'):\n",
        "                  lbout.write(line)\n",
        "                  number_lines_labels = number_lines_labels + 1\n",
        "                  if truncate is True and number_lines_labels==max_lines:\n",
        "                    print(number_lines_labels)\n",
        "                    break\n",
        "              label_path = truncate_label_path     \n",
        "\n",
        "    if tokenized_path is not None:\n",
        "        print(F'Tokenizing Text... {text_path}')\n",
        "        with open(text_path) as fp, open(tokenized_path, 'w') as fout:\n",
        "            for line in tqdm(fp, desc='Tokenizing'):\n",
        "                print(*tokenize(line), file=fout)\n",
        "                number_lines_text = number_lines_text + 1\n",
        "                if truncate is True  and number_lines_text==max_lines:\n",
        "                  break\n",
        "        text_path = tokenized_path\n",
        "\n",
        "    if not os.path.exists(vocab_path):\n",
        "        print(F'Building Vocab. {text_path}')\n",
        "        with open(text_path) as fp:\n",
        "            vocab, emb_init = build_vocab(fp, w2v_model, vocab_size=vocab_size)\n",
        "        np.save(vocab_path, vocab)\n",
        "        np.save(emb_path, emb_init)\n",
        "    vocab = {word: i for i, word in enumerate(np.load(vocab_path))}\n",
        "    print(F'Vocab Size: {len(vocab)}')\n",
        "\n",
        "    print(F'Getting Dataset: {text_path} Max Length: {max_len}')\n",
        "    texts, labels = convert_to_binary(text_path, label_path, max_len, vocab)\n",
        "    print(F'Size of Samples: {len(texts)}')\n",
        "    print(F'Size of Labels: {len(labels)}')\n",
        "    np.save(os.path.splitext(text_path)[0], texts)\n",
        "    if labels is not None:\n",
        "        assert len(texts) == len(labels)\n",
        "        np.save(os.path.splitext(label_path)[0], labels)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8GCzIJfehoip"
      },
      "outputs": [],
      "source": [
        "#  method to load the dataset and the associated labels \n",
        "def get_data(text_file, label_file=None):\n",
        "    return np.load(text_file, allow_pickle=False), np.load(label_file, allow_pickle=True) if label_file is not None else None"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MKLvmRa3UoXM"
      },
      "outputs": [],
      "source": [
        "from torch.utils.data import Dataset\n",
        "from scipy.sparse import csr_matrix\n",
        "from typing import Sequence, Optional\n",
        "\n",
        "TDataX = Sequence[Sequence]\n",
        "TDataY = Optional[csr_matrix]\n",
        "\n",
        "class MultiLabelDataset(Dataset):\n",
        "    def __init__(self, data_x: TDataX, data_y: TDataY = None, training=True):\n",
        "        self.data_x, self.data_y, self.training = data_x, data_y, training\n",
        "\n",
        "    def __getitem__(self, item):\n",
        "        data_x = self.data_x[item]\n",
        "        if self.training and self.data_y is not None:\n",
        "            data_y = self.data_y[item].toarray().squeeze(0).astype(np.float32)\n",
        "            return data_x, data_y\n",
        "        else:\n",
        "            return data_x\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.data_x)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6n1X4MXgCeO7"
      },
      "source": [
        "##  EUR-Lex\n",
        "EUR-Lex dataset is already tokenized in advance so tokenized_path=None,\n",
        "Emb_init and vocabulary  will be created and saved in the following paths, if they have not already been created."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PMw00bWtIC8h"
      },
      "outputs": [],
      "source": [
        "print('Unzip EUR-Lex Dataset and Word Embedding...')\n",
        "\n",
        "!tar -xvzf /content/gdrive/MyDrive/Machine_Learning/EUR-Lex.tar.gz \n",
        "!unzip /content/gdrive/MyDrive/Machine_Learning/glove.zip\n",
        "\n",
        "                                     #####################################################################################################\n",
        "\n",
        "TRAIN_TEXT_PATH = \"/content/EUR-Lex/train_texts.txt\"\n",
        "TRAIN_LABEL_PATH = \"/content/EUR-Lex/train_labels.txt\"\n",
        "TRAIN_W2V_MODEL_PATH = \"/content/glove.840B.300d.gensim\"\n",
        "VOCAB_PATH = \"/content/EUR-Lex/vocabulary.npy\"\n",
        "EMB_PATH = \"/content/EUR-Lex/emb_init.npy\"\n",
        "\n",
        "TEST_TEXT_PATH = \"/content/EUR-Lex/test_texts.txt\"\n",
        "TEST_LABEL_PATH = \"/content/EUR-Lex/test_labels.txt\"\n",
        "TEST_RESULTS_PATH = \"/content/EUR-Lex/results\"\n",
        "\n",
        "\n",
        "LABELS_BINARIZER_PATH = \"/content/EUR-Lex/labels_binarizer\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Nt0K14GhJ3Sw"
      },
      "outputs": [],
      "source": [
        "print('Running Preprocessing...')\n",
        "\n",
        "# Prepocessing del Trainset di EUR-lex\n",
        "preprocessing(text_path=TRAIN_TEXT_PATH,label_path=TRAIN_LABEL_PATH,vocab_path=VOCAB_PATH,emb_path=EMB_PATH,w2v_model=TRAIN_W2V_MODEL_PATH)\n",
        "# Prepocessing del Testset di EUR-lex\n",
        "preprocessing(text_path=TEST_TEXT_PATH,label_path=TEST_LABEL_PATH,vocab_path=VOCAB_PATH)\n",
        "\n",
        "print('Preprocessing Completed!')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0OQNcIkqDoKn"
      },
      "source": [
        "## AmazonCat-13k\n",
        "AmazonCat-13k dataset is not already tokenized in advance so you need to specify tokenized_path."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XHqpZMtQamVL"
      },
      "outputs": [],
      "source": [
        "\n",
        "print('Unzip AmazonCat-13k Dataset and Word Embedding...')\n",
        "\n",
        "\n",
        "!tar -xvzf /content/gdrive/MyDrive/Machine_Learning/AmazonCat-13K.tar.gz\n",
        "!unzip /content/gdrive/MyDrive/Machine_Learning/glove.zip\n",
        "\n",
        "\n",
        "                                     #####################################################################################################\n",
        "\n",
        "\n",
        "TRAIN_TEXT_PATH = \"/content/AmazonCat-13K/train_raw_texts.txt\"\n",
        "TRAIN_TOKENIZED_PATH = \"/content/AmazonCat-13K/train_tokenized_texts.txt\"\n",
        "TRAIN_LABEL_PATH = \"/content/AmazonCat-13K/train_labels.txt\"\n",
        "TRAIN_TRUNCATE_LABEL_PATH = \"/content/AmazonCat-13K/train_truncate_labels.txt\"\n",
        "\n",
        "TRAIN_W2V_MODEL_PATH = \"/content/glove.840B.300d.gensim\"\n",
        "VOCAB_PATH = \"/content/AmazonCat-13K/vocabulary.npy\"\n",
        "EMB_PATH = \"/content/AmazonCat-13K/emb_init.npy\"\n",
        "\n",
        "\n",
        "TEST_TEXT_PATH = \"/content/AmazonCat-13K/test_raw_texts.txt\"\n",
        "TEST_TOKENIZED_PATH = \"/content/AmazonCat-13K/test_tokenized_texts.txt\"\n",
        "TEST_LABEL_PATH = \"/content/AmazonCat-13K/test_labels.txt\"\n",
        "TEST_TRUNCATE_LABEL_PATH = \"/content/AmazonCat-13K/test_truncate_labels.txt\"\n",
        "\n",
        "TEST_RESULTS_PATH = \"/content/AmazonCat-13K/results\"\n",
        "\n",
        "\n",
        "LABELS_BINARIZER_PATH = \"/content/AmazonCat-13K/labels_binarizer\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "O7q1ogmRha7R"
      },
      "outputs": [],
      "source": [
        "print('Running Preprocessing...')\n",
        "\n",
        "# Prepocessing del Trainset di AmazonCat-13k\n",
        "preprocessing(text_path=TRAIN_TEXT_PATH,tokenized_path=TRAIN_TOKENIZED_PATH,label_path=TRAIN_LABEL_PATH,truncate_label_path=TRAIN_TRUNCATE_LABEL_PATH,vocab_path=VOCAB_PATH,emb_path=EMB_PATH,w2v_model=TRAIN_W2V_MODEL_PATH, truncate=True)\n",
        "# Prepocessing del Testset di AmazonCat-13k\n",
        "preprocessing(text_path=TEST_TEXT_PATH,tokenized_path=TEST_TOKENIZED_PATH,label_path=TEST_LABEL_PATH, truncate_label_path=TEST_TRUNCATE_LABEL_PATH, vocab_path=VOCAB_PATH, truncate=True, max_lines=30000)\n",
        "fu\n",
        "print('Preprocessing Completed!')\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gL994HZESglb"
      },
      "source": [
        "# CorNet\n",
        "\n",
        "In this cell is reported th code to create a CorNet block.\n",
        "\n",
        "It is possible to concatente more layers togheter to obatian a deep CorNet\n",
        "module.\n",
        "\n",
        "A CorNet block is a computational unit which maps raw label predictions to enhanced label predictions based on label correlations.\n",
        "\n",
        "Formally, a CorNet building block is defined as:\n",
        "\n",
        "***ð = ð¹ (ð) + ð***\n",
        "\n",
        "where ð, ð are the input and the output of this CorNet block and ð¹ stands for the underlying mapping function. Specifically, ð denotes the raw label predictions before the CorNet block and ð denotes the enhanced label predictions after the CorNet block.\n",
        "\n",
        "ð¹ is the correlation enhancing function to be learned. The most straightforward design for function ð¹ is one fully connected layer:\n",
        "\n",
        " ***ð¹ (ð) = ð¾ð***\n",
        "\n",
        "where ð¾ denotes the weight matrix of the layer, and the bias term and the activation function are omitted for simplifying notations. In this way, the ðð¡h enhanced prediction ð¦ð is a linear combination of all raw predictions\n",
        "*{ð¥1, ð¥2, ..., ð¥ð , ...}* and hence all possible linear correlations between the ðð¡h label and other labels are taken into consideration.\n",
        "\n",
        "We insert a bottleneck layer between ð and ð. Let ð denote the dimension of the bottleneck layer.\n",
        "By having a bottleneck layer and setting ð âª ð.\n",
        "The model size is significantly reduced and more complex correlations can be captured by the additional layer.\n",
        "Therefore, our design for function ð¹ can be formally defined as:\n",
        "\n",
        "***ð¹(ð)=ð¾2ð¿(ð¾1ð(ð)+ð1)+ð2***\n",
        "\n",
        "where ð¾1, ð¾2 are the weight matrices, ð1, ð2 are the biases, and ð, ð¿ are the sigmoid activation function and the ELU activation function respectively. Recall that ð are the raw label prediction logits, hence we first need to use the sigmoid activation to convert label logits ð to label probabilities ð(ð) ranging from 0 to 1, representing the confidence level of each label prediction. The correlations are then calculated based on the label probabilities. By doing so, the interpretability of label correlations is also achieved.\n",
        "\n",
        "\n",
        "\n",
        "---\n",
        "\n",
        "\n",
        "We have assumed that ð is the output label predictions of a deep XMTC network, but actually ð could also be the output label predictions of a previous CorNet block. This means we can stack any number of CorNet blocks to form a deep CorNet module and the output of each block is a correlation enhancement over the output of the previous block."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qN2tIW7kR8NI"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "\n",
        "ACT2FN = {'elu': F.elu, 'relu': F.relu, 'sigmoid': torch.sigmoid, 'tanh': torch.tanh}\n",
        "\n",
        "class CorNetBlock(nn.Module):\n",
        "    def __init__(self, context_size, output_size, cornet_act='sigmoid', **kwargs):\n",
        "        super(CorNetBlock, self).__init__()\n",
        "        self.dstbn2cntxt = nn.Linear(output_size, context_size)\n",
        "        self.cntxt2dstbn = nn.Linear(context_size, output_size)\n",
        "        self.act_fn = ACT2FN[cornet_act]\n",
        "    \n",
        "    def forward(self, output_dstrbtn):        \n",
        "        identity_logits = output_dstrbtn        \n",
        "        output_dstrbtn = self.act_fn(output_dstrbtn) # Sigmoid activation Function \n",
        "        context_vector = self.dstbn2cntxt(output_dstrbtn) # linear layer\n",
        "        context_vector = F.elu(context_vector) # ELU atctivation function\n",
        "        output_dstrbtn = self.cntxt2dstbn(context_vector) # linear layer\n",
        "        output_dstrbtn = output_dstrbtn + identity_logits        \n",
        "        return output_dstrbtn\n",
        "    \n",
        "    \n",
        "class CorNet(nn.Module):\n",
        "    def __init__(self, output_size, cornet_dim=1000, n_cornet_blocks=2, **kwargs):\n",
        "        super(CorNet, self).__init__()\n",
        "        self.intlv_layers = nn.ModuleList([CorNetBlock(cornet_dim, output_size, **kwargs) for _ in range(n_cornet_blocks)])\n",
        "        for layer in self.intlv_layers:\n",
        "            nn.init.xavier_uniform_(layer.dstbn2cntxt.weight)\n",
        "            nn.init.xavier_uniform_(layer.cntxt2dstbn.weight)\n",
        "\n",
        "    def forward(self, logits):        \n",
        "        for layer in self.intlv_layers:\n",
        "            logits = layer(logits)        \n",
        "        return logits"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZGzwtRh7Fccq"
      },
      "source": [
        "#AttentionXML\n",
        "\n",
        "The main AttentionXML components are:\n",
        "\n",
        "1.   Bidirectional LSTMS;\n",
        "2.   Multi-label attention layer.\n",
        "\n",
        "---\n",
        "\n",
        "The goal is to improve the performance of this model by introducing the Module CorNet.\n",
        "\n",
        "Using the correlation, the hope is to improve the assignment of the labels."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Oe6RG7vxMJ8-"
      },
      "source": [
        "### Word Representation\n",
        "The input of AttentionXML is raw tokenized text with length TË. Each word is represented by a deep semantic dense vector, called word embedding.\n",
        "\n",
        "In this experiments, we use pre-trained 300-dimensional ***GloVe word embedding*** as our initial word representation.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hDxntYnVQXUa"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "\n",
        "class Embedding(nn.Module):\n",
        "    def __init__(self, vocab_size=None, emb_size=None, emb_init=None, emb_trainable=True, padding_idx=0, dropout=0.2):\n",
        "        super(Embedding, self).__init__()\n",
        "        if emb_init is not None:\n",
        "            vocab_size, emb_size = emb_init.shape\n",
        "        self.emb = nn.Embedding(vocab_size, emb_size, padding_idx=padding_idx, sparse=True,\n",
        "                                  _weight=torch.from_numpy(emb_init).float() if emb_init is not None else None)\n",
        "        self.emb.weight.requires_grad = emb_trainable\n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "        self.padding_idx = padding_idx\n",
        "\n",
        "    def forward(self, inputs):\n",
        "        emb_out = self.dropout(self.emb(inputs))\n",
        "        lengths, masks = (inputs != self.padding_idx).sum(dim=-1), inputs != self.padding_idx\n",
        "        return emb_out[:, :lengths.max()], lengths, masks[:, :lengths.max()]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_bQ5jdClSjrl"
      },
      "source": [
        "### Bidirectional LSTM\n",
        "We use a Bidirectional LSTM (BiLSTM) to capture\n",
        "both the left and right-sides context, where at each time step t the output h_t is obtained by concatenating the forward output h_tâ and the backward output h_tâ."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "x9gB4Q9nU1Ov"
      },
      "outputs": [],
      "source": [
        "class LSTMEncoder(nn.Module):\n",
        "    def __init__(self, input_size, hidden_size, layers_num, dropout):\n",
        "        super(LSTMEncoder, self).__init__()\n",
        "        self.lstm = nn.LSTM(input_size, hidden_size, layers_num, batch_first=True, bidirectional=True)\n",
        "        self.init_state = nn.Parameter(torch.zeros(2*2*layers_num, 1, hidden_size))\n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "\n",
        "    def forward(self, inputs, lengths, **kwargs):\n",
        "        self.lstm.flatten_parameters()\n",
        "        init_state = self.init_state.repeat([1, inputs.size(0), 1]) #repeats along the second direction for the number input_size\n",
        "        cell_init, hidden_init = init_state[:init_state.size(0)//2], init_state[init_state.size(0)//2:]\n",
        "        idx = torch.argsort(lengths, descending=True)\n",
        "        packed_inputs = nn.utils.rnn.pack_padded_sequence(inputs[idx], lengths[idx].to('cpu'), batch_first=True) #    PERCHE VENGO IMPACCHETTATI E DISPACHETTATI \n",
        "        outputs, _ = nn.utils.rnn.pad_packed_sequence(\n",
        "            self.lstm(packed_inputs, (hidden_init, cell_init))[0], batch_first=True)\n",
        "        return self.dropout(outputs[torch.argsort(idx)])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oqJwb7Bgivyx"
      },
      "source": [
        "###Attention\n",
        "Recently, an attention mechanism in neural networks has been successfully used in many NLP tasks, such as machine translation, machine comprehension, relation extraction, and speech recognition. The most relevant context to each label can be different in XMTC. \n",
        "\n",
        "AttentionXML computes the (linear) combination of context vectors hËi for each label through a multi-label attention mechanism, to capture various intensive parts of a text."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "IyRv88xNU5YX"
      },
      "outputs": [],
      "source": [
        "class MLAttention(nn.Module):\n",
        "    def __init__(self, labels_num, hidden_size):\n",
        "        super(MLAttention, self).__init__()\n",
        "        self.attention = nn.Linear(hidden_size, labels_num, bias=False)\n",
        "        nn.init.xavier_uniform_(self.attention.weight)\n",
        "\n",
        "    def forward(self, inputs, masks):\n",
        "        masks = torch.unsqueeze(masks, 1)  # N, 1, L\n",
        "        attention = self.attention(inputs).transpose(1, 2).masked_fill(~masks, -np.inf)  # N, labels_num, L\n",
        "        attention = F.softmax(attention, -1)\n",
        "        return attention @ inputs   # N, labels_num, hidden_size"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XVI101Prm-50"
      },
      "source": [
        "AttentionXML has one (or two) fully connected layers and one output layer. The same parameter values are used for all labels at the fully connected (and output) layers, to emphasize differences of attention among all labels. Also sharing the parameter values of fully connected layers among all labels can largely reduce the number of parameters to avoid overfitting and keep the model scale small."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "iLQPvhnwVPLb"
      },
      "outputs": [],
      "source": [
        "class MLLinear(nn.Module):\n",
        "    def __init__(self, linear_size, output_size):\n",
        "        super(MLLinear, self).__init__()\n",
        "        self.linear = nn.ModuleList(nn.Linear(in_s, out_s) for in_s, out_s in zip(linear_size[:-1], linear_size[1:]))\n",
        "        for linear in self.linear:\n",
        "            nn.init.xavier_uniform_(linear.weight)\n",
        "        self.output = nn.Linear(linear_size[-1], output_size)\n",
        "        nn.init.xavier_uniform_(self.output.weight)\n",
        "\n",
        "    def forward(self, inputs):\n",
        "        linear_out = inputs\n",
        "        for linear in self.linear:\n",
        "            linear_out = F.relu(linear(linear_out))\n",
        "        return torch.squeeze(self.output(linear_out), -1)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nNRbCLDbnnc4"
      },
      "source": [
        "Now let's create the AttentionXML model on which we will then add the CorNet module."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NN0h1HJaVT0C"
      },
      "outputs": [],
      "source": [
        "class AttentionXML(nn.Module):\n",
        "    def __init__(self, labels_num, emb_size, hidden_size, layers_num, linear_size, dropout, \n",
        "                 vocab_size=None, emb_init=None, emb_trainable=True, padding_idx=0, emb_dropout=0.2, **kwargs):\n",
        "        super(AttentionXML, self).__init__()\n",
        "        self.emb = Embedding(vocab_size, emb_size, emb_init, emb_trainable, padding_idx, emb_dropout)\n",
        "        self.lstm = LSTMEncoder(emb_size, hidden_size, layers_num, dropout)\n",
        "        self.attention = MLAttention(labels_num, hidden_size * 2)\n",
        "        self.linear = MLLinear([hidden_size * 2] + linear_size, 1)\n",
        "\n",
        "    def forward(self, inputs, **kwargs):\n",
        "        emb_out, lengths, masks = self.emb(inputs, **kwargs)\n",
        "        rnn_out = self.lstm(emb_out, lengths)   # N, L, hidden_size * 2\n",
        "        attn_out = self.attention(rnn_out, masks)      # N, labels_num, hidden_size * 2\n",
        "        return self.linear(attn_out)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "w8HvEAqnVXuz"
      },
      "outputs": [],
      "source": [
        "class CorNetAttentionXML(nn.Module):\n",
        "    def __init__(self, labels_num, emb_size, hidden_size, layers_num, linear_size, dropout, **kwargs):\n",
        "        super(CorNetAttentionXML, self).__init__()\n",
        "        self.attnrnn = AttentionXML(labels_num, emb_size, hidden_size, layers_num, linear_size, dropout, **kwargs)\n",
        "        self.cornet = CorNet(labels_num, **kwargs)\n",
        "\n",
        "    def forward(self, input_variables):\n",
        "        raw_logits = self.attnrnn(input_variables)\n",
        "        cor_logits = self.cornet(raw_logits)        \n",
        "        return cor_logits"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "K5sC4XZr_m22"
      },
      "source": [
        "#Optimizer\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VX7tmbK_AiMn"
      },
      "outputs": [],
      "source": [
        "import math\n",
        "import torch\n",
        "from torch.optim.optimizer import Optimizer\n",
        "\n",
        "\n",
        "class DenseSparseAdam(Optimizer):\n",
        "    \"\"\"\n",
        "    \"\"\"\n",
        "    def __init__(self, params, lr=1e-3, betas=(0.9, 0.999), eps=1e-8, weight_decay=0.0):\n",
        "        if not 0.0 <= lr:\n",
        "            raise ValueError(\"Invalid learning rate: {}\".format(lr))\n",
        "        if not 0.0 <= eps:\n",
        "            raise ValueError(\"Invalid epsilon value: {}\".format(eps))\n",
        "        if not 0.0 <= betas[0] < 1.0:\n",
        "            raise ValueError(\"Invalid beta parameter at index 0: {}\".format(betas[0]))\n",
        "        if not 0.0 <= betas[1] < 1.0:\n",
        "            raise ValueError(\"Invalid beta parameter at index 1: {}\".format(betas[1]))\n",
        "        defaults = dict(lr=lr, betas=betas, eps=eps, weight_decay=weight_decay)\n",
        "        super(DenseSparseAdam, self).__init__(params, defaults)\n",
        "\n",
        "    def step(self, closure=None):\n",
        "        \"\"\"\n",
        "        Performs a single optimization step.\n",
        "        Parameters\n",
        "        ----------\n",
        "        closure : ``callable``, optional.\n",
        "            A closure that reevaluates the model and returns the loss.\n",
        "        \"\"\"\n",
        "        loss = None\n",
        "        if closure is not None:\n",
        "            loss = closure()\n",
        "\n",
        "        for group in self.param_groups:\n",
        "            for p in group['params']:\n",
        "                if p.grad is None:\n",
        "                    continue\n",
        "                grad = p.grad.data\n",
        "\n",
        "                state = self.state[p]\n",
        "\n",
        "                # State initialization\n",
        "                if 'step' not in state:\n",
        "                    state['step'] = 0\n",
        "                if 'exp_avg' not in state:\n",
        "                    # Exponential moving average of gradient values\n",
        "                    state['exp_avg'] = torch.zeros_like(p.data)\n",
        "                if 'exp_avg_sq' not in state:\n",
        "                    # Exponential moving average of squared gradient values\n",
        "                    state['exp_avg_sq'] = torch.zeros_like(p.data)\n",
        "\n",
        "                state['step'] += 1\n",
        "\n",
        "                exp_avg, exp_avg_sq = state['exp_avg'], state['exp_avg_sq']\n",
        "                beta1, beta2 = group['betas']\n",
        "\n",
        "                weight_decay = group['weight_decay']\n",
        "\n",
        "                if grad.is_sparse:\n",
        "                    grad = grad.coalesce()  # the update is non-linear so indices must be unique\n",
        "                    grad_indices = grad._indices()\n",
        "                    grad_values = grad._values()\n",
        "                    size = grad.size()\n",
        "\n",
        "                    def make_sparse(values):\n",
        "                        constructor = grad.new\n",
        "                        if grad_indices.dim() == 0 or values.dim() == 0:\n",
        "                            return constructor().resize_as_(grad)\n",
        "                        return constructor(grad_indices, values, size)\n",
        "\n",
        "                    # Decay the first and second moment running average coefficient\n",
        "                    #      old <- b * old + (1 - b) * new\n",
        "                    # <==> old += (1 - b) * (new - old)\n",
        "                    old_exp_avg_values = exp_avg.sparse_mask(grad)._values()\n",
        "                    exp_avg_update_values = grad_values.sub(old_exp_avg_values).mul_(1 - beta1)\n",
        "                    exp_avg.add_(make_sparse(exp_avg_update_values))\n",
        "                    old_exp_avg_sq_values = exp_avg_sq.sparse_mask(grad)._values()\n",
        "                    exp_avg_sq_update_values = grad_values.pow(2).sub_(old_exp_avg_sq_values).mul_(1 - beta2)\n",
        "                    exp_avg_sq.add_(make_sparse(exp_avg_sq_update_values))\n",
        "\n",
        "                    # Dense addition again is intended, avoiding another sparse_mask\n",
        "                    numer = exp_avg_update_values.add_(old_exp_avg_values)\n",
        "                    exp_avg_sq_update_values.add_(old_exp_avg_sq_values)\n",
        "                    denom = exp_avg_sq_update_values.sqrt_().add_(group['eps'])\n",
        "                    del exp_avg_update_values, exp_avg_sq_update_values\n",
        "\n",
        "                    bias_correction1 = 1 - beta1 ** state['step']\n",
        "                    bias_correction2 = 1 - beta2 ** state['step']\n",
        "                    step_size = group['lr'] * math.sqrt(bias_correction2) / bias_correction1\n",
        "\n",
        "                    p.data.add_(make_sparse(-step_size * numer.div_(denom)))\n",
        "                    if weight_decay > 0.0:\n",
        "                        p.data.add_(-group['lr'] * weight_decay, p.data.sparse_mask(grad))\n",
        "                else:\n",
        "                    # Decay the first and second moment running average coefficient\n",
        "                    exp_avg.mul_(beta1).add_(1 - beta1, grad)\n",
        "                    exp_avg_sq.mul_(beta2).addcmul_(1 - beta2, grad, grad)\n",
        "                    denom = exp_avg_sq.sqrt().add_(group['eps'])\n",
        "\n",
        "                    bias_correction1 = 1 - beta1 ** state['step']\n",
        "                    bias_correction2 = 1 - beta2 ** state['step']\n",
        "                    step_size = group['lr'] * math.sqrt(bias_correction2) / bias_correction1\n",
        "\n",
        "                    p.data.addcdiv_(-step_size, exp_avg, denom)\n",
        "                    if weight_decay > 0.0:\n",
        "                        p.data.add_(-group['lr'] * weight_decay, p.data)\n",
        "\n",
        "        return loss"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9cPg5McPYFeh"
      },
      "source": [
        "Utility methods that aid in testing."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1cEqKyUUZW21"
      },
      "outputs": [],
      "source": [
        "from scipy.sparse import csr_matrix\n",
        "from sklearn.preprocessing import MultiLabelBinarizer\n",
        "from typing import Union, Optional, List, Iterable, Hashable\n",
        "import numpy as np\n",
        "\n",
        "\n",
        "TPredict = np.ndarray\n",
        "TTarget = Union[Iterable[Iterable[Hashable]], csr_matrix] # variable that can take on both types\n",
        "TMlb = Optional[MultiLabelBinarizer]\n",
        "TClass = Optional[List[Hashable]]\n",
        "\n",
        "\n",
        "\n",
        "def get_mlb_evaluation(classes: TClass = None, mlb: TMlb = None, targets: TTarget = None):\n",
        "    if classes is not None:\n",
        "        mlb = MultiLabelBinarizer(classes, sparse_output=True)\n",
        "    if mlb is None and targets is not None:\n",
        "        if isinstance(targets, csr_matrix):\n",
        "            mlb = MultiLabelBinarizer(classes=range(targets.shape[1]), sparse_output=True)\n",
        "            mlb.fit(None)\n",
        "        else:\n",
        "            mlb = MultiLabelBinarizer(sparse_output=True)\n",
        "            mlb.fit(targets)\n",
        "    return mlb"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CM6jN-xuY0Q_"
      },
      "source": [
        "#Evaluation\n",
        "We adopt two instance-based ranking metrics to evaluate the models: the precision at top k (precision@k) and the normalized Discounted Cumulative Gain at top k (nDCG@k)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "BR9zfyWicA6U"
      },
      "outputs": [],
      "source": [
        "#PRECISION @K\n",
        "\n",
        "from functools import partial\n",
        "\n",
        "def get_precision(prediction: TPredict, targets: TTarget, mlb: TMlb = None, classes: TClass = None, top=5):\n",
        "    mlb = get_mlb_evaluation(classes, mlb, targets)\n",
        "    if not isinstance(targets, csr_matrix):\n",
        "        targets = mlb.transform(targets)\n",
        "    prediction = mlb.transform(prediction[:, :top])\n",
        "    return prediction.multiply(targets).sum() / (top * targets.shape[0])\n",
        "\n",
        "\n",
        "get_p_1 = partial(get_precision, top=1)\n",
        "get_p_3 = partial(get_precision, top=3)\n",
        "get_p_5 = partial(get_precision, top=5)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "b-IwXXcNcBSE"
      },
      "outputs": [],
      "source": [
        "# Normalized Discounted Cumulative Gain\n",
        "\n",
        "\n",
        "def get_ndcg(prediction: TPredict, targets: TTarget, mlb: TMlb = None, classes: TClass = None, top=5):\n",
        "    mlb = get_mlb_evaluation(classes, mlb, targets)\n",
        "    log = 1.0 / np.log2(np.arange(top) + 2) # add 2 because python 0-index\n",
        "    dcg = np.zeros((targets.shape[0], 1))\n",
        "    if not isinstance(targets, csr_matrix):\n",
        "        targets = mlb.transform(targets)\n",
        "    for i in range(top):\n",
        "        p = mlb.transform(prediction[:, i: i+1])\n",
        "        dcg += p.multiply(targets).sum(axis=-1) * log[i]\n",
        "    return np.average(dcg / log.cumsum()[np.minimum(targets.sum(axis=-1), top) - 1])\n",
        "\n",
        "\n",
        "get_n_1 = partial(get_ndcg, top=1)\n",
        "get_n_3 = partial(get_ndcg, top=3)\n",
        "get_n_5 = partial(get_ndcg, top=5)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "N5-Szl0GbZHq"
      },
      "source": [
        "Method that recalls the two metrics we use to compare the model with respect to the other configurations.\n",
        "\n",
        "They are compared as the k value varies, which varies between 1-3-5."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "HDLpFN76lekk"
      },
      "outputs": [],
      "source": [
        "from sklearn.preprocessing import MultiLabelBinarizer\n",
        "\n",
        "def testing(results, targets, train_labels):\n",
        "    res, targets = np.load(results, allow_pickle=True), np.load(targets, allow_pickle=True)\n",
        "    mlb = MultiLabelBinarizer(sparse_output=True)\n",
        "    targets = mlb.fit_transform(targets)\n",
        "    print('Precision@1,3,5:', get_p_1(res, targets, mlb), get_p_3(res, targets, mlb), get_p_5(res, targets, mlb))\n",
        "    print('nDCG@1,3,5:', get_n_1(res, targets, mlb), get_n_3(res, targets, mlb), get_n_5(res, targets, mlb))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WmYm2cpabiZF"
      },
      "source": [
        "#Model\n",
        "Class that manages the model and perform training and testing"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "J_RovlAs_kOc"
      },
      "outputs": [],
      "source": [
        "from collections import deque\n",
        "from typing import Optional, Mapping\n",
        "from torch.utils.data import DataLoader\n",
        "\n",
        "\n",
        "class Model(object):\n",
        "    def __init__(self, network, model_path, gradient_clip_value=5.0, device_ids=None, **kwargs):\n",
        "        self.model = nn.DataParallel(network(**kwargs).cuda(), device_ids=device_ids)\n",
        "        self.loss_fn = nn.BCEWithLogitsLoss() # loss function\n",
        "        self.model_path, self.state = model_path, {}\n",
        "        os.makedirs(os.path.split(self.model_path)[0], exist_ok=True)\n",
        "        self.gradient_clip_value, self.gradient_norm_queue = gradient_clip_value, deque([np.inf], maxlen=5)\n",
        "        self.optimizer = None\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "# set of the optimizer\n",
        "    def get_optimizer(self, **kwargs):\n",
        "        self.optimizer = DenseSparseAdam(self.model.parameters(), **kwargs)\n",
        "\n",
        "\n",
        "    def swa_init(self):\n",
        "      if 'swa' not in self.state:\n",
        "          print('SWA Initializing')\n",
        "          swa_state = self.state['swa'] = {'models_num': 1}\n",
        "          for n, p in self.model.named_parameters(): # Returns an iterator over module parameters, yielding both the name of the parameter as well as the parameter itself.\n",
        "              swa_state[n] = p.data.cpu().detach()    \n",
        "  \n",
        "    def swa_step(self):\n",
        "        if 'swa' in self.state:\n",
        "            swa_state = self.state['swa']\n",
        "            swa_state['models_num'] += 1\n",
        "            beta = 1.0 / swa_state['models_num']\n",
        "            with torch.no_grad():\n",
        "                for n, p in self.model.named_parameters():\n",
        "                    swa_state[n].mul_(1.0 - beta).add_(beta, p.data.cpu())\n",
        "\n",
        "    def swap_swa_params(self):\n",
        "        if 'swa' in self.state:\n",
        "            swa_state = self.state['swa']\n",
        "            for n, p in self.model.named_parameters():\n",
        "                gpu_id = p.get_device()\n",
        "                p.data, swa_state[n] = swa_state[n], p.data.cpu()\n",
        "                p.data = p.data.cuda(gpu_id)\n",
        "\n",
        "  # Methods to save and load model \n",
        "\n",
        "    def save_model(self, last_epoch):\n",
        "      if not last_epoch: return\n",
        "      for trial in range(5):\n",
        "          try:                \n",
        "              torch.save(self.model.module.state_dict(), self.model_path)\n",
        "              break\n",
        "          except:\n",
        "              print('saving failed')\n",
        "\n",
        "    def load_model(self):\n",
        "        self.model.module.load_state_dict(torch.load(self.model_path))             \n",
        "\n",
        "\n",
        "# Method to manage the problems of exploding gradients and vanishing gradients\n",
        "    def clip_gradient(self):\n",
        "        if self.gradient_clip_value is not None:\n",
        "            max_norm = max(self.gradient_norm_queue)\n",
        "            total_norm = torch.nn.utils.clip_grad_norm_(self.model.parameters(), max_norm * self.gradient_clip_value)\n",
        "            self.gradient_norm_queue.append(min(total_norm, max_norm * 2.0, 1.0))\n",
        "            if total_norm > max_norm * self.gradient_clip_value:\n",
        "                print(F'Clipping gradients with total norm {total_norm} '\n",
        "                            F'and max norm {max_norm}')\n",
        "\n",
        "\n",
        "\n",
        "# step of the Training Phase\n",
        "\n",
        "    def train_step(self, train_x: torch.Tensor, train_y: torch.Tensor):\n",
        "        self.optimizer.zero_grad()\n",
        "        self.model.train()\n",
        "        scores = self.model(train_x)\n",
        "        loss = self.loss_fn(scores, train_y)\n",
        "        loss.backward()\n",
        "        #self.clip_gradient()\n",
        "        self.optimizer.step(closure=None)\n",
        "        return loss.item()\n",
        "\n",
        "# step of the Testing Phase\n",
        "   \n",
        "    def predict_step(self, data_x: torch.Tensor, k: int):\n",
        "        self.model.eval()\n",
        "        with torch.no_grad():\n",
        "            scores, labels = torch.topk(self.model(data_x), k)\n",
        "            return torch.sigmoid(scores).cpu(), labels.cpu()\n",
        "\n",
        "\n",
        "# method of the training\n",
        "    def train(self, train_loader: DataLoader, valid_loader: DataLoader, opt_params: Optional[Mapping] = None,\n",
        "              nb_epoch=100, step=100, k=5, early=100, verbose=True, swa_warmup=None, **kwargs):\n",
        "      \n",
        "        self.get_optimizer(**({} if opt_params is None else opt_params))\n",
        "        global_step, best_n5, e = 0, 0.0, 0\n",
        "        print_loss = 0.0\n",
        "        for epoch_idx in range(nb_epoch):\n",
        "            print(F'Epoch Number: {epoch_idx}')\n",
        "            if epoch_idx == swa_warmup:\n",
        "                self.swa_init()\n",
        "            for i, (train_x, train_y) in enumerate(train_loader, 1):\n",
        "                global_step += 1\n",
        "                loss = self.train_step(train_x, train_y.cuda())\n",
        "                print_loss += loss\n",
        "                if global_step % step == 0:\n",
        "                    self.swa_step()\n",
        "                    self.swap_swa_params()\n",
        "                    labels = []\n",
        "                    valid_loss = 0.0\n",
        "                    self.model.eval()\n",
        "                    with torch.no_grad():\n",
        "                        for (valid_x, valid_y) in valid_loader:\n",
        "                            logits = self.model(valid_x)\n",
        "                            valid_loss += self.loss_fn(logits, valid_y.cuda()).item()\n",
        "                            scores, tmp = torch.topk(logits, k)\n",
        "                            labels.append(tmp.cpu())\n",
        "                    valid_loss /= len(valid_loader)\n",
        "                    labels = np.concatenate(labels)\n",
        "                    targets = valid_loader.dataset.data_y\n",
        "                    p5, n5 = get_p_5(labels, targets), get_n_5(labels, targets) \n",
        "                    if n5 > best_n5:\n",
        "                        self.save_model(True)#epoch_idx > 1 * swa_warmup)\n",
        "                        best_n5, e = n5, 0\n",
        "                    else:\n",
        "                        e += 1\n",
        "                        if early is not None and e > early:\n",
        "                            return\n",
        "                    self.swap_swa_params()\n",
        "                    if verbose:\n",
        "                        log_msg = '%d %d train loss: %.7f valid loss: %.7f P@5: %.5f N@5: %.5f early stop: %d' % \\\n",
        "                        (epoch_idx, i * train_loader.batch_size, print_loss / step, valid_loss, round(p5, 5), round(n5, 5), e)\n",
        "                        print(log_msg)\n",
        "                        print_loss = 0.0\n",
        "\n",
        "\n",
        "\n",
        "    def predict(self, data_loader: DataLoader, k=100, desc='Predict', **kwargs):\n",
        "        self.load_model()\n",
        "        scores_list, labels_list = zip(*(self.predict_step(data_x, k) for data_x in tqdm(data_loader, desc=desc, leave=False)))\n",
        "        return np.concatenate(scores_list), np.concatenate(labels_list)\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1xvttGu7gWgO"
      },
      "outputs": [],
      "source": [
        "# method to save the results obtained in a specific path\n",
        "\n",
        "def output_res(output_path, name, scores, labels):\n",
        "    os.makedirs(output_path, exist_ok=True)\n",
        "    np.save(os.path.join(output_path, F'{name}-scores'), scores)\n",
        "    np.save(os.path.join(output_path, F'{name}-labels'), labels)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mobRw-MN1hqT"
      },
      "source": [
        "#Main Function\n",
        "This method takes care of managing all the model and dataset that we want to use.\n",
        "\n",
        "In particular, it will load the datasets (Traing-Validation-Test), then the model and finally training and testing it.\n",
        "\n",
        "Run the cell corresponding to the dataset you are using. \n",
        "The corresponding dictionary will be passed into the main method via **kwargs taking the dictionary keys and values.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-jKJ7al7GO-6"
      },
      "source": [
        "### CorNetAttentionXML-EUR-Lex Parameters and Paths"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "R4MnIhUp-FxH"
      },
      "outputs": [],
      "source": [
        "# CorNetAttentionXML-EUR-Lex Parameters\n",
        "\n",
        "model_param = {\"hidden_size\": 256, \"layers_num\": 1, \"linear_size\": [256], \"dropout\": 0.5, \"emb_trainable\": False}\n",
        "data_param = {\"emb_size\" : 300}\n",
        "predict_param = {\"batch_size\" : 40}\n",
        "\n",
        "VALID_SIZE=200"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "A3yF9Nj7WTZ4"
      },
      "outputs": [],
      "source": [
        "# CorNetAttentionXML-EUR-Lex Paths\n",
        "TRAIN_TEXT_PATH_NPY = \"/content/EUR-Lex/train_texts.npy\"\n",
        "TRAIN_LABEL_PATH_NPY = \"/content/EUR-Lex/train_labels.npy\"\n",
        "\n",
        "\n",
        "TEST_TEXT_PATH_NPY = \"/content/EUR-Lex/test_texts.npy\"\n",
        "TEST_LABEL_PATH_NPY= \"/content/EUR-Lex/test_labels.npy\"\n",
        "\n",
        "\n",
        "EURLEX_PREDICTION_RESULTS= '/content/EUR-Lex/results/CorNetAttentionXML-EUR-Lex-labels.npy'\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "q6qZu24bF_Ku"
      },
      "source": [
        "###CorNetAttentionXML-AmazonCat-13K Parmeters and Paths"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mCtu2WWL9Di0"
      },
      "outputs": [],
      "source": [
        "# CorNetAttentionXML-AmazonCat-13K Parmeters\n",
        "\n",
        "model_param = {\"hidden_size\": 200, \"layers_num\": 1, \"linear_size\": [100], \"dropout\": 0.5}\n",
        "data_param = {\"emb_size\" : 300}\n",
        "predict_param = {\"batch_size\" : 400}\n",
        "\n",
        "VALID_SIZE=4000"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "clPFt20QFnKl"
      },
      "outputs": [],
      "source": [
        "# CorNetAttentionXML-AmazonCat-13K Paths\n",
        "TRAIN_TEXT_PATH_NPY = \"/content/AmazonCat-13K/train_tokenized_texts.npy\"\n",
        "TRAIN_LABEL_PATH_NPY = \"/content/AmazonCat-13K/train_truncate_labels.npy\"\n",
        "\n",
        "\n",
        "TEST_TEXT_PATH_NPY = \"/content/AmazonCat-13K/test_tokenized_texts.npy\"\n",
        "TEST_LABEL_PATH_NPY= \"/content/AmazonCat-13K/test_truncate_labels.npy\"\n",
        "\n",
        "\n",
        "AMAZON_CAT_13K_PREDICTION_RESULTS= '/content/AmazonCat-13K/results/CorNetAttentionXML-AmazonCat-13K-labels.npy'\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dLG17-PGF4QF"
      },
      "source": [
        "##Main Method"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "UR3XPNB4n57i"
      },
      "outputs": [],
      "source": [
        "\n",
        "from torch.utils.data import DataLoader\n",
        "import os\n",
        "from pathlib import Path\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "\n",
        "\n",
        "def main( valid_size, model_name, data_name, nb_epoch, mode='train'):\n",
        "    #model_path = F'/content/EUR-Lex/{model_name}_{data_name}' # in this way the model is saved based on the dataset that we are using\n",
        "    model_path = F'/content/gdrive/MyDrive/Machine_Learning/{model_name}_{data_name}'\n",
        "    emb_init = get_word_emb(EMB_PATH) # path of the embedding\n",
        "    model= None\n",
        "\n",
        "    print(F'Model Name: {model_name}')\n",
        "    \n",
        "    if mode is None or mode == 'train':\n",
        "      # loading current dataset\n",
        "        print('Loading Training and Validation Set')\n",
        "        train_x, train_labels = get_data(TRAIN_TEXT_PATH_NPY, TRAIN_LABEL_PATH_NPY)\n",
        "        random_state = 1240\n",
        "        train_x, valid_x, train_labels, valid_labels = train_test_split(train_x, train_labels,test_size=valid_size,random_state=random_state)\n",
        "        mlb = get_mlb(LABELS_BINARIZER_PATH, np.hstack((train_labels, valid_labels)))\n",
        "        train_y, valid_y = mlb.transform(train_labels), mlb.transform(valid_labels)\n",
        "\n",
        "\n",
        "        labels_num = len(mlb.classes_)\n",
        "        print(F'Number of Labels: {labels_num}')\n",
        "        print(F'Size of Training Set: {len(train_x)}')\n",
        "        print(F'Size of Validation Set: {len(valid_x)}')\n",
        "        print('Training')\n",
        "\n",
        "\n",
        "        \"\"\"\n",
        "        \n",
        "         Eurlex - Train_batch_size: 40, Valid_batch_size: 40, Predict_batch_size: 40\n",
        "         AmazonCat-13K - Train_batch_size: 200, Valid_batch_size: 200, Predict_batch_size: 200\n",
        "\n",
        "        \"\"\"\n",
        "\n",
        "\n",
        "        train_loader = DataLoader(MultiLabelDataset(train_x, train_y),batch_size=40, shuffle=True, num_workers=4)\n",
        "        valid_loader = DataLoader(MultiLabelDataset(valid_x, valid_y, training=True),batch_size=40, num_workers=4)\n",
        "        model = Model(network=CorNetAttentionXML, labels_num=labels_num, model_path=model_path, emb_init=emb_init, **data_param, **model_param)\n",
        "      \n",
        "        model.train(train_loader, valid_loader, nb_epoch=nb_epoch, swa_warmup=10) \n",
        "        print('Finish Training')\n",
        "\n",
        "# section for evaluation\n",
        "    if mode is None or mode == 'eval':\n",
        "      print('Loading Test Set')\n",
        "      mlb = get_mlb(LABELS_BINARIZER_PATH)\n",
        "      labels_num = len(mlb.classes_)\n",
        "      test_x, _ = get_data(TEST_TEXT_PATH_NPY, None)\n",
        "      print(F'Size of Test Set: {len(test_x)}')\n",
        "\n",
        "      print('Predicting')\n",
        "      test_loader = DataLoader(MultiLabelDataset(test_x), batch_size=40, num_workers=4)\n",
        "      if model is None:\n",
        "          model = Model(network=CorNetAttentionXML, labels_num=labels_num, model_path=model_path, emb_init=emb_init, **data_param, **model_param)\n",
        "      scores, labels = model.predict(test_loader, k=predict_param.get('k', 100))\n",
        "      print('Finish Predicting')\n",
        "      labels = mlb.classes_[labels]\n",
        "      output_res(TEST_RESULTS_PATH, F'{model_name}-{data_name}', scores, labels)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hBC-e7Hrzu-4"
      },
      "outputs": [],
      "source": [
        "# TRAINING\n",
        "main(valid_size=VALID_SIZE, model_name='CorNetAttentionXML', data_name='EUR-Lex',nb_epoch=30, mode='train')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GM4Vpdot5rx9"
      },
      "outputs": [],
      "source": [
        "# PREDICTING AND TESTING\n",
        "main(valid_size=VALID_SIZE, model_name='CorNetAttentionXML', data_name='AmazonCat-13K', nb_epoch=1, mode='eval')\n",
        "\n",
        "testing(results=AMAZON_CAT_13K_PREDICTION_RESULTS, targets= TEST_LABEL_PATH_NPY, train_labels=TRAIN_LABEL_PATH_NPY)\n"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}